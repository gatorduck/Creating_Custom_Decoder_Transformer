{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49801d9b",
   "metadata": {},
   "source": [
    "# Text Decoder Transformer - Natural Language Version\n",
    "\n",
    "A decoder-only transformer for natural language text generation using Keras best practices. Adapted from the healthcare version with larger sequence lengths for text corpus training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de17492",
   "metadata": {},
   "source": [
    "## Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b29ed977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Keras version: 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers, losses, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e5ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text length: 1101961 characters\n",
      "Total sequences: 8104\n",
      "Example sequence: first citizen: before we proceed any further, hear me speak. all: speak, speak. first citizen: you a...\n",
      "Unique words (approx): 23101\n"
     ]
    }
   ],
   "source": [
    "# Download and prepare text corpus (Shakespeare as example)\n",
    "def download_text_corpus(url=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"):\n",
    "    \"\"\"Download a text corpus for training.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        text = response.text\n",
    "    except:\n",
    "        # Fallback to a simple text if download fails\n",
    "        text = \"\"\"To be or not to be, that is the question. Whether 'tis nobler in the mind to suffer \n",
    "        the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and by opposing end them.\n",
    "        The quick brown fox jumps over the lazy dog. A journey of a thousand miles begins with a single step.\n",
    "        In the beginning was the Word, and the Word was with God, and the Word was God.\n",
    "        All that glitters is not gold. Actions speak louder than words. Better late than never.\n",
    "        \"\"\" * 100  # Repeat to create more text\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Download corpus\n",
    "corpus_text = download_text_corpus()\n",
    "\n",
    "# Clean and prepare text\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Keep basic punctuation\n",
    "    text = re.sub(r'[^a-z0-9\\s.,!?;:-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "clean_corpus = clean_text(corpus_text)\n",
    "\n",
    "# Create text sequences\n",
    "def create_text_sequences(text, seq_length=50, overlap=25):\n",
    "    \"\"\"Create overlapping text sequences.\"\"\"\n",
    "    words = text.split()\n",
    "    sequences = []\n",
    "    \n",
    "    for i in range(0, len(words) - seq_length, overlap):\n",
    "        sequence = ' '.join(words[i:i + seq_length])\n",
    "        if len(sequence.split()) == seq_length:\n",
    "            sequences.append(sequence)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "text_sequences = create_text_sequences(clean_corpus, seq_length=50, overlap=25)\n",
    "\n",
    "print(f\"Total text length: {len(clean_corpus)} characters\")\n",
    "print(f\"Total sequences: {len(text_sequences)}\")\n",
    "print(f\"Example sequence: {text_sequences[0][:100]}...\")\n",
    "print(f\"Unique words (approx): {len(set(clean_corpus.split()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd235e",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76067756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified Model Configuration (to avoid overflow):\n",
      "  vocab_size: 5000\n",
      "  max_len: 32\n",
      "  embed_dim: 128\n",
      "  num_heads: 4\n",
      "  ff_dim: 256\n",
      "  num_layers: 3\n",
      "  dropout_rate: 0.1\n",
      "  batch_size: 8\n",
      "  epochs: 20\n",
      "  learning_rate: 0.0001\n",
      "\n",
      "Estimated model parameters: ~738,304\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters - simplified to avoid overflow issues\n",
    "CONFIG = {\n",
    "    'vocab_size': 5000,         # Reduced vocabulary size\n",
    "    'max_len': 32,              # Reduced sequence length\n",
    "    'embed_dim': 128,           # Smaller embeddings\n",
    "    'num_heads': 4,             # Fewer attention heads\n",
    "    'ff_dim': 256,              # Smaller feed-forward dimension\n",
    "    'num_layers': 3,            # Fewer layers\n",
    "    'dropout_rate': 0.1,        # Keep regularization\n",
    "    'batch_size': 8,            # Smaller batch size\n",
    "    'epochs': 20,               # Fewer epochs\n",
    "    'learning_rate': 1e-4       # Standard learning rate\n",
    "}\n",
    "\n",
    "print(\"Simplified Model Configuration (to avoid overflow):\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nEstimated model parameters: ~{CONFIG['embed_dim'] * CONFIG['vocab_size'] + CONFIG['num_layers'] * CONFIG['embed_dim'] * CONFIG['ff_dim']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71cd11",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c9582e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5000\n",
      "Training batches: 1013\n",
      "Example vocab: ['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "# Create text vectorization layer for natural language\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=CONFIG['vocab_size'],\n",
    "    output_sequence_length=CONFIG['max_len'] + 1,\n",
    "    output_mode='int',\n",
    "    standardize='lower_and_strip_punctuation'  # Better for natural language\n",
    ")\n",
    "\n",
    "# Create dataset and adapt vectorizer\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_sequences)\n",
    "vectorizer.adapt(text_ds.batch(CONFIG['batch_size']))\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "\n",
    "def prepare_sequences(texts):\n",
    "    \"\"\"Prepare input-target pairs for autoregressive training.\"\"\"\n",
    "    texts = tf.expand_dims(texts, -1)\n",
    "    tokenized = vectorizer(texts)\n",
    "    inputs = tokenized[:, :-1]  # All tokens except last\n",
    "    targets = tokenized[:, 1:]  # All tokens except first (shifted)\n",
    "    return inputs, targets\n",
    "\n",
    "# Create training dataset with optimizations for larger sequences\n",
    "train_ds = text_ds.shuffle(buffer_size=1000) \\\n",
    "    .batch(CONFIG['batch_size']) \\\n",
    "    .map(prepare_sequences, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Training batches: {len(list(train_ds))}\")\n",
    "print(f\"Example vocab: {vocab[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4ba1b",
   "metadata": {},
   "source": [
    "## Custom Layers (Best Practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7330489",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.utils.register_keras_serializable()\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    \"\"\"Combines token and positional embeddings with scaling.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, max_len, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))  # Scale embeddings\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.token_embedding = layers.Embedding(\n",
    "            self.vocab_size, \n",
    "            self.embed_dim,\n",
    "            embeddings_initializer='uniform'  # Better initialization for text\n",
    "        )\n",
    "        self.position_embedding = layers.Embedding(self.max_len, self.embed_dim)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(seq_len)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "        token_embeds = self.token_embedding(inputs) * self.scale  # Scale token embeddings\n",
    "        return token_embeds + position_embeds\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'max_len': self.max_len\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "class DecoderBlock(layers.Layer):\n",
    "    \"\"\"Enhanced transformer decoder block for natural language.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Multi-head self-attention with larger key dimension\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=self.embed_dim // self.num_heads,  # Standard key dimension\n",
    "            dropout=self.dropout_rate,\n",
    "            use_bias=True\n",
    "        )\n",
    "        \n",
    "        # Enhanced feed-forward network with ReLU activation (more stable on Windows)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(self.ff_dim, activation='relu'),  # Using ReLU instead of GELU for stability\n",
    "            layers.Dropout(self.dropout_rate),\n",
    "            layers.Dense(self.embed_dim)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization and dropout\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(self.dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(self.dropout_rate)\n",
    "        \n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Create causal mask (more efficient for longer sequences)\n",
    "        i = tf.range(seq_len)[:, None]\n",
    "        j = tf.range(seq_len)\n",
    "        causal_mask = i >= j\n",
    "        \n",
    "        # Self-attention with causal masking\n",
    "        attn_output = self.attention(\n",
    "            inputs, inputs, attention_mask=causal_mask, training=training\n",
    "        )\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799e24f",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03201c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"text_decoder_transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"text_decoder_transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_tokens (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_embedding_3          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">644,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)           │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">132,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">132,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">132,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_45          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vocab_projection (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">645,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_tokens (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ positional_embedding_3          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m644,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)           │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_75 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_18 (\u001b[38;5;33mDecoderBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m132,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_19 (\u001b[38;5;33mDecoderBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m132,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_block_20 (\u001b[38;5;33mDecoderBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m132,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_45          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vocab_projection (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m5000\u001b[0m)       │       \u001b[38;5;34m645,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,792</span> (6.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,686,792\u001b[0m (6.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,792</span> (6.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,686,792\u001b[0m (6.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total parameters: 1,686,792\n"
     ]
    }
   ],
   "source": [
    "def create_text_decoder_transformer(config):\n",
    "    \"\"\"Create a decoder-only transformer model for text generation.\"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=(config['max_len'],), dtype=tf.int32, name='text_tokens')\n",
    "    \n",
    "    # Positional embeddings with scaling\n",
    "    x = PositionalEmbedding(\n",
    "        vocab_size=config['vocab_size'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        max_len=config['max_len']\n",
    "    )(inputs)\n",
    "    \n",
    "    # Add input dropout\n",
    "    x = layers.Dropout(config['dropout_rate'])(x)\n",
    "    \n",
    "    # Stack decoder blocks\n",
    "    for i in range(config['num_layers']):\n",
    "        x = DecoderBlock(\n",
    "            embed_dim=config['embed_dim'],\n",
    "            num_heads=config['num_heads'],\n",
    "            ff_dim=config['ff_dim'],\n",
    "            dropout_rate=config['dropout_rate']\n",
    "        )(x)\n",
    "    \n",
    "    # Final layer normalization\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Output projection\n",
    "    outputs = layers.Dense(config['vocab_size'], name='vocab_projection')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='text_decoder_transformer')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_text_decoder_transformer(CONFIG)\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5347730",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f3c973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified training setup for text generation\n",
    "# Removed WarmupLearningRateSchedule to avoid potential overflow issues\n",
    "\n",
    "# Compile model with simplified optimizer to avoid overflow\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(  # Using standard Adam for stability\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7  # Slightly larger epsilon for numerical stability\n",
    "    ),\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy', 'sparse_top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Enhanced callbacks for text training\n",
    "training_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        '../models/text_transformer_best.keras',\n",
    "        monitor='loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ac230",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0841799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text transformer training with manual loop...\n",
      "Training on 8104 sequences with max length 32\n",
      "\n",
      "Epoch 1/5\n",
      "Step 100, Loss: 7.9357, Accuracy: 0.0465\n",
      "Step 100, Loss: 7.9357, Accuracy: 0.0465\n",
      "Step 200, Loss: 7.4438, Accuracy: 0.0467\n",
      "Step 200, Loss: 7.4438, Accuracy: 0.0467\n",
      "Step 300, Loss: 7.1446, Accuracy: 0.0466\n",
      "Step 300, Loss: 7.1446, Accuracy: 0.0466\n",
      "Step 400, Loss: 6.9716, Accuracy: 0.0471\n",
      "Step 400, Loss: 6.9716, Accuracy: 0.0471\n",
      "Step 500, Loss: 6.8599, Accuracy: 0.0463\n",
      "Step 500, Loss: 6.8599, Accuracy: 0.0463\n",
      "Step 600, Loss: 6.7778, Accuracy: 0.0464\n",
      "Step 600, Loss: 6.7778, Accuracy: 0.0464\n",
      "Step 700, Loss: 6.7135, Accuracy: 0.0470\n",
      "Step 700, Loss: 6.7135, Accuracy: 0.0470\n",
      "Step 800, Loss: 6.6542, Accuracy: 0.0471\n",
      "Step 800, Loss: 6.6542, Accuracy: 0.0471\n",
      "Step 900, Loss: 6.6085, Accuracy: 0.0475\n",
      "Step 900, Loss: 6.6085, Accuracy: 0.0475\n",
      "Step 1000, Loss: 6.5651, Accuracy: 0.0478\n",
      "Step 1000, Loss: 6.5651, Accuracy: 0.0478\n",
      "Epoch 1 - Loss: 6.5594, Accuracy: 0.0478\n",
      "\n",
      "Epoch 2/5\n",
      "Epoch 1 - Loss: 6.5594, Accuracy: 0.0478\n",
      "\n",
      "Epoch 2/5\n",
      "Step 100, Loss: 6.0979, Accuracy: 0.0471\n",
      "Step 100, Loss: 6.0979, Accuracy: 0.0471\n",
      "Step 200, Loss: 6.0696, Accuracy: 0.0464\n",
      "Step 200, Loss: 6.0696, Accuracy: 0.0464\n",
      "Step 300, Loss: 6.0694, Accuracy: 0.0457\n",
      "Step 300, Loss: 6.0694, Accuracy: 0.0457\n",
      "Step 400, Loss: 6.0653, Accuracy: 0.0467\n",
      "Step 400, Loss: 6.0653, Accuracy: 0.0467\n",
      "Step 500, Loss: 6.0538, Accuracy: 0.0465\n",
      "Step 500, Loss: 6.0538, Accuracy: 0.0465\n",
      "Step 600, Loss: 6.0372, Accuracy: 0.0464\n",
      "Step 600, Loss: 6.0372, Accuracy: 0.0464\n",
      "Step 700, Loss: 6.0158, Accuracy: 0.0470\n",
      "Step 700, Loss: 6.0158, Accuracy: 0.0470\n",
      "Step 800, Loss: 5.9958, Accuracy: 0.0471\n",
      "Step 800, Loss: 5.9958, Accuracy: 0.0471\n",
      "Step 900, Loss: 5.9756, Accuracy: 0.0475\n",
      "Step 900, Loss: 5.9756, Accuracy: 0.0475\n",
      "Step 1000, Loss: 5.9581, Accuracy: 0.0478\n",
      "Step 1000, Loss: 5.9581, Accuracy: 0.0478\n",
      "Epoch 2 - Loss: 5.9555, Accuracy: 0.0478\n",
      "\n",
      "Epoch 3/5\n",
      "Epoch 2 - Loss: 5.9555, Accuracy: 0.0478\n",
      "\n",
      "Epoch 3/5\n",
      "Step 100, Loss: 5.8052, Accuracy: 0.0505\n",
      "Step 100, Loss: 5.8052, Accuracy: 0.0505\n",
      "Step 200, Loss: 5.7767, Accuracy: 0.0477\n",
      "Step 200, Loss: 5.7767, Accuracy: 0.0477\n",
      "Step 300, Loss: 5.7727, Accuracy: 0.0468\n",
      "Step 300, Loss: 5.7727, Accuracy: 0.0468\n",
      "Step 400, Loss: 5.7757, Accuracy: 0.0473\n",
      "Step 500, Loss: 5.7686, Accuracy: 0.0462\n",
      "Step 600, Loss: 5.7610, Accuracy: 0.0462\n",
      "Step 700, Loss: 5.7513, Accuracy: 0.0466\n",
      "Step 800, Loss: 5.7354, Accuracy: 0.0470\n",
      "Step 900, Loss: 5.7229, Accuracy: 0.0471\n",
      "Step 1000, Loss: 5.7109, Accuracy: 0.0476\n",
      "Epoch 3 - Loss: 5.7088, Accuracy: 0.0478\n",
      "\n",
      "Epoch 4/5\n",
      "Step 100, Loss: 5.6057, Accuracy: 0.0507\n",
      "Step 200, Loss: 5.5892, Accuracy: 0.0480\n",
      "Step 300, Loss: 5.5896, Accuracy: 0.0467\n",
      "Step 400, Loss: 5.5976, Accuracy: 0.0470\n",
      "Step 500, Loss: 5.6025, Accuracy: 0.0461\n",
      "Step 600, Loss: 5.5973, Accuracy: 0.0463\n",
      "Step 700, Loss: 5.5913, Accuracy: 0.0467\n",
      "Step 800, Loss: 5.5761, Accuracy: 0.0472\n",
      "Step 900, Loss: 5.5652, Accuracy: 0.0474\n",
      "Step 1000, Loss: 5.5581, Accuracy: 0.0477\n",
      "Epoch 4 - Loss: 5.5566, Accuracy: 0.0478\n",
      "\n",
      "Epoch 5/5\n",
      "Step 100, Loss: 5.4932, Accuracy: 0.0496\n",
      "Step 200, Loss: 5.4684, Accuracy: 0.0476\n",
      "Step 300, Loss: 5.4581, Accuracy: 0.0470\n",
      "Step 400, Loss: 5.4700, Accuracy: 0.0469\n",
      "Step 500, Loss: 5.4719, Accuracy: 0.0463\n",
      "Step 600, Loss: 5.4712, Accuracy: 0.0463\n",
      "Step 700, Loss: 5.4678, Accuracy: 0.0469\n",
      "Step 800, Loss: 5.4553, Accuracy: 0.0472\n",
      "Step 900, Loss: 5.4456, Accuracy: 0.0475\n",
      "Step 1000, Loss: 5.4393, Accuracy: 0.0478\n",
      "Epoch 5 - Loss: 5.4379, Accuracy: 0.0478\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final loss: 5.4379\n",
      "✅ Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Manual training loop without problematic accuracy metric\n",
    "print(\"Starting text transformer training with manual loop...\")\n",
    "print(f\"Training on {len(text_sequences)} sequences with max length {CONFIG['max_len']}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=CONFIG['learning_rate'])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training metrics - only using loss to avoid overflow\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "\n",
    "# History tracking\n",
    "history_loss = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Custom accuracy calculation function (avoiding problematic ops)\n",
    "def calculate_accuracy(targets, predictions):\n",
    "    \"\"\"Calculate accuracy manually to avoid TensorFlow overflow issues.\"\"\"\n",
    "    # Get predicted tokens (simple max without argmax)\n",
    "    pred_tokens = tf.cast(tf.math.reduce_max(predictions, axis=-1) > 0, tf.int32)\n",
    "    target_tokens = tf.cast(targets, tf.int32)\n",
    "    \n",
    "    # Simple match counting\n",
    "    matches = tf.cast(tf.equal(pred_tokens, target_tokens), tf.float32)\n",
    "    accuracy = tf.reduce_mean(matches)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "epochs = min(CONFIG['epochs'], 5)  # Limit epochs for initial test\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Reset metrics\n",
    "    train_loss.reset_state()\n",
    "    epoch_accuracy_sum = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Training step\n",
    "    step = 0\n",
    "    for batch in train_ds:\n",
    "        inputs, targets = batch\n",
    "        loss = train_step(inputs, targets)\n",
    "        \n",
    "        # Calculate accuracy manually for this batch\n",
    "        with tf.GradientTape():\n",
    "            predictions = model(inputs, training=False)\n",
    "        batch_acc = calculate_accuracy(targets, predictions)\n",
    "        epoch_accuracy_sum += float(batch_acc)\n",
    "        batch_count += 1\n",
    "        \n",
    "        step += 1\n",
    "        if step % 100 == 0:\n",
    "            current_acc = epoch_accuracy_sum / batch_count if batch_count > 0 else 0.0\n",
    "            print(f\"Step {step}, Loss: {train_loss.result():.4f}, Accuracy: {current_acc:.4f}\")\n",
    "    \n",
    "    # Record history\n",
    "    epoch_loss = float(train_loss.result())\n",
    "    epoch_acc = epoch_accuracy_sum / batch_count if batch_count > 0 else 0.0\n",
    "    history_loss.append(epoch_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal loss: {history_loss[-1]:.4f}\")\n",
    "print(\"✅ Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b39a44",
   "metadata": {},
   "source": [
    "## Model Saving & End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Models saved successfully!\n",
      "📊 Model parameters: 1,686,792\n",
      "📚 Vocabulary size: 5000\n",
      "📏 Max sequence length: 32\n",
      "💾 Saved files:\n",
      "  - text_transformer_compact.keras (base model)\n",
      "  - text_transformer_inference.keras (end-to-end model)\n"
     ]
    }
   ],
   "source": [
    "def create_text_inference_model(trained_model, vectorizer):\n",
    "    \"\"\"Create end-to-end model for text generation.\"\"\"\n",
    "    \n",
    "    # Text input\n",
    "    text_input = layers.Input(shape=(), dtype=tf.string, name='input_text')\n",
    "    \n",
    "    # Vectorize and prepare for transformer\n",
    "    vectorized = vectorizer(text_input)\n",
    "    transformer_input = vectorized[:, :-1]  # Remove last token for input\n",
    "    \n",
    "    # Apply trained transformer\n",
    "    predictions = trained_model(transformer_input)\n",
    "    \n",
    "    # Create inference model\n",
    "    inference_model = models.Model(\n",
    "        inputs=text_input,\n",
    "        outputs=predictions,\n",
    "        name='text_transformer_inference'\n",
    "    )\n",
    "    \n",
    "    return inference_model\n",
    "\n",
    "# Create inference model\n",
    "inference_model = create_text_inference_model(model, vectorizer)\n",
    "\n",
    "# Save models\n",
    "model.save('../models/text_transformer_compact.keras')\n",
    "inference_model.save('../models/text_transformer_inference.keras')\n",
    "\n",
    "# Note: TextVectorization layer is already included in the inference model\n",
    "# The vocabulary is accessible via vectorizer.get_vocabulary()\n",
    "\n",
    "\n",
    "print(f\"Model parameters: {model.count_params():,}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Max sequence length: {CONFIG['max_len']}\")\n",
    "print(\"Saved files:\")\n",
    "print(\"  - text_transformer_compact.keras (base model)\")\n",
    "print(\"  - text_transformer_inference.keras (end-to-end model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544fb27c",
   "metadata": {},
   "source": [
    "## Text Generation & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4c9d08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Text Generation Examples\n",
      "============================================================\n",
      "\n",
      "Prompt: 'To be or not to be'\n",
      "Top 3 next word predictions:\n",
      "  1. a: 0.0538\n",
      "  2. and: 0.0334\n",
      "  3. i: 0.0286\n",
      "Generated continuation: your but o or tranio if no or so that petruchio katharina your petruchio you\n",
      "\n",
      "Prompt: 'The quick brown fox'\n",
      "Top 3 next word predictions:\n",
      "  1. of: 0.0651\n",
      "  2. and: 0.0489\n",
      "  3. a: 0.0260\n",
      "Generated continuation: your but o or tranio if no or so that petruchio katharina your petruchio you\n",
      "\n",
      "Prompt: 'The quick brown fox'\n",
      "Top 3 next word predictions:\n",
      "  1. of: 0.0651\n",
      "  2. and: 0.0489\n",
      "  3. a: 0.0260\n",
      "Generated continuation: tis the the if if have katharina in for katharina sir that hortensio here with\n",
      "\n",
      "Prompt: 'In the beginning was'\n",
      "Top 3 next word predictions:\n",
      "  1. of: 0.0888\n",
      "  2. a: 0.0356\n",
      "  3. and: 0.0340\n",
      "Generated continuation: tis the the if if have katharina in for katharina sir that hortensio here with\n",
      "\n",
      "Prompt: 'In the beginning was'\n",
      "Top 3 next word predictions:\n",
      "  1. of: 0.0888\n",
      "  2. a: 0.0356\n",
      "  3. and: 0.0340\n",
      "Generated continuation: is my the what\n",
      "\n",
      "Prompt: 'All that glitters'\n",
      "Top 3 next word predictions:\n",
      "  1. a: 0.0228\n",
      "  2. and: 0.0212\n",
      "Generated continuation: is my the what\n",
      "\n",
      "Prompt: 'All that glitters'\n",
      "Top 3 next word predictions:\n",
      "  1. a: 0.0228\n",
      "  2. and: 0.0212\n",
      "Generated continuation: katharina the were o\n",
      "\n",
      "Text transformer ready for natural language generation!\n",
      "Generated continuation: katharina the were o\n",
      "\n",
      "Text transformer ready for natural language generation!\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, model, vectorizer, vocab, max_length=50, temperature=0.8, top_k=40):\n",
    "    \"\"\"Generate text continuation given a prompt.\"\"\"\n",
    "    \n",
    "    current_text = prompt.lower().strip()\n",
    "    generated_words = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Vectorize current text\n",
    "        vectorized = vectorizer([current_text])\n",
    "        input_tokens = vectorized[0, :-1]  # Remove last token\n",
    "        input_tokens = tf.expand_dims(input_tokens, 0)\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = model(input_tokens, training=False)\n",
    "        \n",
    "        # Get probabilities for next token position\n",
    "        seq_len = len(current_text.split())\n",
    "        if seq_len >= logits.shape[1]:\n",
    "            seq_len = logits.shape[1] - 1\n",
    "            \n",
    "        next_token_logits = logits[0, seq_len] / temperature\n",
    "        \n",
    "        # Apply top-k filtering\n",
    "        top_k_logits, top_k_indices = tf.nn.top_k(next_token_logits, k=top_k)\n",
    "        probabilities = tf.nn.softmax(top_k_logits)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        sampled_index = tf.random.categorical([probabilities], 1)[0, 0]\n",
    "        predicted_id = top_k_indices[sampled_index]\n",
    "        \n",
    "        # Convert to word\n",
    "        if predicted_id < len(vocab):\n",
    "            predicted_word = vocab[predicted_id]\n",
    "            if predicted_word and predicted_word not in ['', '[UNK]']:\n",
    "                generated_words.append(predicted_word)\n",
    "                current_text += f\" {predicted_word}\"\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        # Keep only recent context to avoid exceeding max length\n",
    "        words = current_text.split()\n",
    "        if len(words) > CONFIG['max_len'] - 5:\n",
    "            current_text = ' '.join(words[-(CONFIG['max_len'] - 10):])\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "def predict_next_word(text, model, vectorizer, vocab, top_k=5):\n",
    "    \"\"\"Predict next words for a given text.\"\"\"\n",
    "    \n",
    "    # Vectorize input\n",
    "    vectorized = vectorizer([text.lower()])\n",
    "    input_tokens = vectorized[0, :-1]  # Remove last token\n",
    "    input_tokens = tf.expand_dims(input_tokens, 0)\n",
    "    \n",
    "    # Get predictions\n",
    "    logits = model(input_tokens, training=False)\n",
    "    \n",
    "    # Get probabilities for next token position\n",
    "    seq_len = len(text.split())\n",
    "    if seq_len >= logits.shape[1]:\n",
    "        seq_len = logits.shape[1] - 1\n",
    "        \n",
    "    next_token_logits = logits[0, seq_len]\n",
    "    probabilities = tf.nn.softmax(next_token_logits)\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_indices = tf.nn.top_k(probabilities, k=top_k).indices.numpy()\n",
    "    top_probs = tf.nn.top_k(probabilities, k=top_k).values.numpy()\n",
    "    \n",
    "    # Convert to words\n",
    "    predictions = []\n",
    "    for idx, prob in zip(top_indices, top_probs):\n",
    "        if idx < len(vocab):\n",
    "            word = vocab[idx]\n",
    "            if word and word not in ['', '[UNK]']:\n",
    "                predictions.append((word, float(prob)))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test text generation\n",
    "test_prompts = [\n",
    "    \"To be or not to be\",\n",
    "    \"The quick brown fox\",\n",
    "    \"In the beginning was\",\n",
    "    \"All that glitters\"\n",
    "]\n",
    "\n",
    "print(\" Text Generation Examples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    # Show next word predictions\n",
    "    predictions = predict_next_word(prompt, model, vectorizer, vocab, top_k=3)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"Top 3 next word predictions:\")\n",
    "    for i, (word, prob) in enumerate(predictions, 1):\n",
    "        print(f\"  {i}. {word}: {prob:.4f}\")\n",
    "    \n",
    "    # Generate continuation\n",
    "    generated = generate_text(prompt, model, vectorizer, vocab, max_length=15, temperature=0.7)\n",
    "    print(f\"Generated continuation: {generated}\")\n",
    "\n",
    "print(\"\\nText transformer ready for natural language generation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
